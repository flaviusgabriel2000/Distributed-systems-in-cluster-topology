// Distributed systems in cluster topology


	The general idea of my code to solve the task was to
use the functionalities provided by MPI so that all processes
know the topology of the system, and the coordinator processes
are able to distribute the vector into their clusters.

			REZOLVAREA TASKURILOR

	Initially, each coordinator process opens its own input file
according to its rank, so that it can learn about its worker processes
in the cluster and build part of its topology. This is represented as an
NxM matrix, where N is the number of coordinators (in the context of the
project, it will always be 4), and M is the total number of processes in
the distributed system. The next step is to send a message to the workers
so that they know who their leader is.
	Next, the topology establishment process practically begins.
Coordinator 0 sends process 1 (or 3 in case of communication error) the list
of workers in its cluster. Process 1 (or 3) overwrites the line corresponding
to process 0 in the topology matrix with what it has just received and will
transmit to the next node the expanded list of workers with the information
it had up to that moment. The mechanism continues until the complete topology
matrix reaches node 0. Coordinator 0 distributes it once more so that all 
processes in the system (including workers) have it. Each one displays the
topology on the screen.
	The calculations are carried out in the following way: process 0
generates the vector, distributes it to the workers in its cluster, receives
back the modified piece that each worker has handled and overwrites
it over what it had until now. Then, it transmits the partially modified vector
to the next coordinator, and this one proceeds identically to process 0.
The algorithm continues until the final vector reaches where it initially started,
and node 0 will display it on the screen.
	In the case of a communication error between process 0 and 1, establishing
the topology and performing the calculations proceeds similarly, except that the
information traverses the "defective" ring in a trigonometric sense and not in a
clockwise direction as in the first part of the project.
	Each MPI_Send operation is followed by a log in stdout with the source and
destination of the message so that there is no communication between nodes not directly
connected.